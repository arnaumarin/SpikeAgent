{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VLM Merge Analysis Tutorial\n",
        "\n",
        "This tutorial demonstrates how to use Vision-Language Models (VLM) to analyze whether units should be merged.\n",
        "\n",
        "## What is VLM Merge Analysis?\n",
        "\n",
        "VLM merge analysis uses AI vision models to determine if pairs or groups of units should be merged into a single unit. This is useful when spike sorting creates multiple units from the same neuron.\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- A `SortingAnalyzer` with computed extensions:\n",
        "  - `waveforms` / `templates` (for waveform plots)\n",
        "  - `spike_locations` (for spike location plots)\n",
        "  - `spike_amplitudes` (for amplitude plots)\n",
        "  - `principal_components` (for PCA clustering)\n",
        "  - `correlograms` (for crosscorrelograms)\n",
        "\n",
        "- API key for your chosen VLM provider (OpenAI, Anthropic, or Google)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "_root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "sys.path.insert(0, os.path.join(_root_dir, 'src'))\n",
        "\n",
        "import spikeinterface as si\n",
        "import numpy as np\n",
        "from spikeagent.app.tool.si_custom import create_merge_img_df\n",
        "from spikeagent.curation.vlm_merge import run_vlm_merge, plot_merge_results\n",
        "from spikeagent.app.tool.utils import get_model\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "print(\"Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Create a SortingAnalyzer\n",
        "\n",
        "We'll create a synthetic `SortingAnalyzer` for demonstration. In practice, you would load your own recording and sorting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Creating synthetic recording and sorting...\")\n",
        "recording, sorting = si.generate_ground_truth_recording(\n",
        "    durations=[10.0],\n",
        "    num_channels=16,\n",
        "    num_units=6,\n",
        "    sampling_frequency=30000.0,\n",
        "    noise_kwargs={'noise_levels': 5.0, 'strategy': 'on_the_fly'},\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "sorting_analyzer = si.create_sorting_analyzer(\n",
        "    sorting=sorting,\n",
        "    recording=recording,\n",
        "    format=\"memory\"\n",
        ")\n",
        "\n",
        "print(f\"Created sorting_analyzer with {len(sorting_analyzer.unit_ids)} units\")\n",
        "print(f\"Unit IDs: {list(sorting_analyzer.unit_ids)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Compute Required Extensions\n",
        "\n",
        "We need to compute all the extensions that VLM merge analysis will use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Computing extensions...\")\n",
        "\n",
        "sorting_analyzer.compute(\"random_spikes\")\n",
        "sorting_analyzer.compute(\"waveforms\", n_jobs=1)\n",
        "sorting_analyzer.compute(\"templates\")\n",
        "sorting_analyzer.compute(\"correlograms\", window_ms=100.0, bin_ms=1.0)\n",
        "sorting_analyzer.compute(\"spike_locations\", method=\"center_of_mass\")\n",
        "sorting_analyzer.compute(\"spike_amplitudes\")\n",
        "sorting_analyzer.compute(\"principal_components\", n_components=3, mode='by_channel_local')\n",
        "\n",
        "available_exts = [ext for ext in sorting_analyzer.get_computable_extensions() \n",
        "                  if sorting_analyzer.has_extension(ext)]\n",
        "print(f\"Computed extensions: {available_exts}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Find Potential Merge Candidates\n",
        "\n",
        "We'll use SpikeInterface's `compute_merge_unit_groups` to automatically find units that might be merged based on template similarity. This is the same approach SpikeAgent uses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from spikeinterface.curation import compute_merge_unit_groups\n",
        "\n",
        "print(\"Computing potential merge candidates based on template similarity...\")\n",
        "steps = [\"template_similarity\"]\n",
        "steps_params = {\n",
        "    \"template_similarity\": {\"template_diff_thresh\": 0.2}\n",
        "}\n",
        "\n",
        "potential_merge_groups = compute_merge_unit_groups(\n",
        "    sorting_analyzer,\n",
        "    resolve_graph=False,\n",
        "    steps_params=steps_params,\n",
        "    steps=steps\n",
        ")\n",
        "\n",
        "num_groups = len(potential_merge_groups)\n",
        "print(f\"Found {num_groups} potential merge groups.\")\n",
        "if num_groups > 0:\n",
        "    print(\"Potential merge groups:\")\n",
        "    for i, group in enumerate(potential_merge_groups[:10]):\n",
        "        print(f\"  Group {i}: {group}\")\n",
        "    if num_groups > 10:\n",
        "        print(f\"  ... and {num_groups - 10} more groups\")\n",
        "else:\n",
        "    print(\"No potential merge groups found. You may want to adjust template_diff_thresh.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create Merge Image DataFrame\n",
        "\n",
        "The VLM needs images comparing units within each group. We'll create a dataframe containing base64-encoded images for each merge group.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(potential_merge_groups) == 0:\n",
        "    print(\"No merge candidates found. Skipping VLM merge analysis.\")\n",
        "    print(\"You can try adjusting template_diff_thresh or manually define merge groups.\")\n",
        "else:\n",
        "    features = [\"waveform_single\", \"amplitude_plot\", \"crosscorrelograms\", \"pca_clustering\"]\n",
        "    \n",
        "    print(f\"\\nCreating merge image dataframe with features: {features}...\")\n",
        "    import tempfile\n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        img_df = create_merge_img_df(\n",
        "            sorting_analyzer,\n",
        "            unit_groups=potential_merge_groups,\n",
        "            features=features,\n",
        "            load_if_exists=False,\n",
        "            save_folder=tmpdir\n",
        "        )\n",
        "    \n",
        "    print(f\"Created merge image dataframe: {img_df.shape}\")\n",
        "    print(f\"Merge groups: {len(img_df)}\")\n",
        "    print(f\"Features: {list(img_df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Run VLM Merge Analysis\n",
        "\n",
        "Now we'll use a Vision-Language Model to analyze whether each potential merge group should actually be merged.\n",
        "\n",
        "**Note:** This requires an API key. Set it in your environment or `.env` file:\n",
        "- `OPENAI_API_KEY` for GPT-4o\n",
        "- `ANTHROPIC_API_KEY` for Claude\n",
        "- `GOOGLE_API_KEY` for Gemini\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(potential_merge_groups) == 0:\n",
        "    print(\"Skipping VLM merge analysis - no merge candidates found.\")\n",
        "    results_df = None\n",
        "else:\n",
        "    model_name = \"gpt-4o\"\n",
        "    model = get_model(model_name)\n",
        "    \n",
        "    print(f\"Initialized model: {model_name}\")\n",
        "    \n",
        "    print(\"\\nRunning VLM merge analysis...\")\n",
        "    print(\"This may take a few minutes depending on the number of groups...\")\n",
        "    \n",
        "    results_df = run_vlm_merge(\n",
        "        model=model,\n",
        "        merge_unit_groups=potential_merge_groups,\n",
        "        img_df=img_df,\n",
        "        features=features,\n",
        "        good_merge_groups=[],\n",
        "        bad_merge_groups=[],\n",
        "        num_workers=10\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nVLM merge analysis complete!\")\n",
        "    print(f\"\\nResults summary:\")\n",
        "    print(f\"Total groups analyzed: {len(results_df)}\")\n",
        "    print(f\"Recommended merges: {len(results_df[results_df['merge_type'] == 'merge'])}\")\n",
        "    print(f\"Recommended to keep separate: {len(results_df[results_df['merge_type'] == 'not merge'])}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: View Results\n",
        "\n",
        "Let's examine the results and see which groups were recommended for merging.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if results_df is not None:\n",
        "    print(\"VLM Merge Analysis Results:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(results_df[['merge_type', 'merge_units']].to_string())\n",
        "    \n",
        "    merge_groups = results_df[results_df['merge_type'] == 'merge'].index.tolist()\n",
        "    keep_separate_groups = results_df[results_df['merge_type'] == 'not merge'].index.tolist()\n",
        "    \n",
        "    print(f\"\\nGroups recommended for merging: {merge_groups}\")\n",
        "    print(f\"Groups recommended to keep separate: {keep_separate_groups}\")\n",
        "    \n",
        "    print(\"\\nDetailed reasoning:\")\n",
        "    for group_idx in range(min(5, len(potential_merge_groups))):\n",
        "        if group_idx in results_df.index:\n",
        "            group = potential_merge_groups[group_idx]\n",
        "            print(f\"\\nGroup {group_idx} (units {group}):\")\n",
        "            print(f\"Decision: {results_df.loc[group_idx, 'merge_type']}\")\n",
        "            reasoning = results_df.loc[group_idx, 'reasoning']\n",
        "            print(f\"Reasoning: {reasoning[:300]}...\")\n",
        "else:\n",
        "    print(\"No results to display - no merge candidates were found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Visualize Results\n",
        "\n",
        "Plot the merge groups with their classification results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if results_df is not None:\n",
        "    plot_merge_results(results_df, img_df)\n",
        "else:\n",
        "    print(\"No results to plot - no merge candidates were found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Apply Merges\n",
        "\n",
        "If merges were recommended, we can apply them using SpikeInterface's merge functionality.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if results_df is not None:\n",
        "    merge_groups = results_df[results_df['merge_type'] == 'merge'].index.tolist()\n",
        "    \n",
        "    if len(merge_groups) > 0:\n",
        "        from spikeinterface.curation.curation_tools import resolve_merging_graph\n",
        "        \n",
        "        merge_unit_pairs = [results_df.loc[group_idx, 'merge_units'] for group_idx in merge_groups]\n",
        "        final_merge_groups = resolve_merging_graph(sorting_analyzer.sorting, merge_unit_pairs)\n",
        "        \n",
        "        print(\"Applying merges...\")\n",
        "        print(f\"Final merge groups: {final_merge_groups}\")\n",
        "        \n",
        "        if final_merge_groups and len(final_merge_groups) > 0:\n",
        "            merged_analyzer = sorting_analyzer.merge_units(\n",
        "                merge_unit_groups=final_merge_groups,\n",
        "                sparsity_overlap=0\n",
        "            )\n",
        "            print(f\"Created merged_analyzer with {len(merged_analyzer.unit_ids)} units\")\n",
        "            print(f\"Original units: {len(sorting_analyzer.unit_ids)}\")\n",
        "            print(f\"Merged units: {len(merged_analyzer.unit_ids)}\")\n",
        "        else:\n",
        "            print(\"No merges to apply after resolving merge graph.\")\n",
        "    else:\n",
        "        print(\"No merges recommended. All units should be kept separate.\")\n",
        "else:\n",
        "    print(\"No results available - no merge candidates were found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This tutorial demonstrated:\n",
        "1. Creating a `SortingAnalyzer` with required extensions\n",
        "2. Using `compute_merge_unit_groups` to automatically find merge candidates (same as SpikeAgent)\n",
        "3. Generating merge image dataframes for VLM analysis\n",
        "4. Running VLM merge analysis to classify merge groups\n",
        "5. Viewing and visualizing results\n",
        "6. Applying merges using `resolve_merging_graph` and `merge_units`\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Try different features: `[\"waveform_multi\", \"spike_locations\"]`\n",
        "- Use few-shot learning by providing `good_merge_groups` and `bad_merge_groups`\n",
        "- Adjust `template_diff_thresh` to find more or fewer merge candidates\n",
        "- Save results: `results_df.to_csv('vlm_merge_results.csv')`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
