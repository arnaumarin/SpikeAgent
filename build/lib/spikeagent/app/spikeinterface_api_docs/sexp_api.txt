API for module: spikeinterface.exporters

Function: export_report(sorting_analyzer, output_folder, remove_if_exists=False, format='png', show_figures=False, peak_sign='neg', force_computation=False, **job_kwargs)
  Docstring:
    Exports a SI spike sorting report. The report includes summary figures of the spike sorting output.
    What is plotted depends on what has been calculated. Unit locations and unit waveforms are always included.
    Unit waveform densities, correlograms and spike amplitudes are plotted if `waveforms`, `correlograms`,
    and `spike_amplitudes` have been computed for the given `sorting_analyzer`.
    
    Parameters
    ----------
    sorting_analyzer : SortingAnalyzer
        A SortingAnalyzer object
    output_folder : str
        The output folder where the report files are saved
    remove_if_exists : bool, default: False
        If True and the output folder exists, it is removed
    format : str, default: "png"
        The output figure format (any format handled by matplotlib)
    peak_sign : "neg" or "pos", default: "neg"
        used to compute amplitudes and metrics
    show_figures : bool, default: False
        If True, figures are shown. If False, figures are closed after saving
    force_computation :  bool, default: False
        Force or not some heavy computaion before exporting
    **job_kwargs : keyword arguments for parallel processing:
            * chunk_duration or chunk_size or chunk_memory or total_memory
                - chunk_size : int
                    Number of samples per chunk
                - chunk_memory : str
                    Memory usage for each job (e.g. "100M", "1G", "500MiB", "2GiB")
                - total_memory : str
                    Total memory usage (e.g. "500M", "2G")
                - chunk_duration : str or float or None
                    Chunk duration in s if float or with units if str (e.g. "1s", "500ms")
            * n_jobs : int | float
                Number of jobs to use. With -1 the number of jobs is the same as number of cores.
                Using a float between 0 and 1 will use that fraction of the total cores.
            * progress_bar : bool
                If True, a progress bar is printed
            * mp_context : "fork" | "spawn" | None, default: None
                Context for multiprocessing. It can be None, "fork" or "spawn".
                Note that "fork" is only safely available on LINUX systems

Function: export_to_ibl_gui(sorting_analyzer: 'SortingAnalyzer', output_folder: 'str | Path', lfp_recording: 'BaseRecording | None' = None, rms_win_length_s=3, welch_win_length_samples=16384, psd_chunk_duration_s=1, psd_num_chunks=100, good_units_query: 'str | None' = 'amplitude_median < -40 and isi_violations_ratio < 0.5 and amplitude_cutoff < 0.2', remove_if_exists: 'bool' = False, verbose: 'bool' = True, **job_kwargs)
  Docstring:
    Exports a sorting analyzer to the format required by the `IBL alignment GUI <https://github.com/int-brain-lab/iblapps/wiki>`_.
    
    Parameters
    ----------
    analyzer: SortingAnalyzer
        The sorting analyzer object to use for spike information.
        Should also contain the pre-processed recording to use for AP-band data.
    output_folder: str | Path
        The output folder for the exports.
    lfp_recording: BaseRecording | None, default: None
        The pre-processed recording to use for LFP data. If None, the LFP data is not exported.
    rms_win_length_s: float, default: 3
        The window length in seconds for the RMS calculation (on the LFP data).
    welch_win_length_samples: int, default: 2^14
        The window length in samples for the Welch spectral density computation (on the LFP data).
    psd_chunk_duration_s: float, default: 1
        The chunk duration in seconds for the spectral density calculation (on the LFP data).
    psd_num_chunks: int, default: 100
        The number of chunks to use for the spectral density calculation (on the LFP data).
    remove_if_exists: bool, default: False
        If True and "output_folder" exists, it is removed and overwritten
    verbose: bool, default: True
        If True, output is verbose

Function: export_to_phy(sorting_analyzer: 'SortingAnalyzer', output_folder: 'str | Path', compute_pc_features: 'bool' = True, compute_amplitudes: 'bool' = True, sparsity: 'Optional[ChannelSparsity]' = None, copy_binary: 'bool' = True, remove_if_exists: 'bool' = False, template_mode: 'str' = 'average', add_quality_metrics: 'bool' = True, add_template_metrics: 'bool' = True, additional_properties: 'list | None' = None, dtype: 'Optional[npt.DTypeLike]' = None, verbose: 'bool' = True, use_relative_path: 'bool' = False, **job_kwargs)
  Docstring:
    Exports a sorting analyzer to the phy template-gui format.
    
    Parameters
    ----------
    sorting_analyzer : SortingAnalyzer
        A SortingAnalyzer object
    output_folder : str | Path
        The output folder where the phy template-gui files are saved
    compute_pc_features : bool, default: True
        If True, pc features are computed
    compute_amplitudes : bool, default: True
        If True, waveforms amplitudes are computed
    sparsity : ChannelSparsity or None, default: None
        The sparsity object
    copy_binary : bool, default: True
        If True, the recording is copied and saved in the phy "output_folder"
    remove_if_exists : bool, default: False
        If True and "output_folder" exists, it is removed and overwritten
    template_mode : str, default: "average"
        Parameter "mode" to be given to SortingAnalyzer.get_template()
    add_quality_metrics : bool, default: True
        If True, quality metrics (if computed) are saved as Phy tsv and will appear in the ClusterView.
    add_template_metrics : bool, default: True
        If True, template metrics (if computed) are saved as Phy tsv and will appear in the ClusterView.
    additional_properties : list | None, default: None
        List of additional properties to be saved as Phy tsv and will appear in the ClusterView.
    dtype : dtype or None, default: None
        Dtype to save binary data
    verbose : bool, default: True
        If True, output is verbose
    use_relative_path : bool, default: False
        If True and `copy_binary=True` saves the binary file `dat_path` in the `params.py` relative to `output_folder` (ie `dat_path=r"recording.dat"`). If `copy_binary=False`, then uses a path relative to the `output_folder`
        If False, uses an absolute path in the `params.py` (ie `dat_path=r"path/to/the/recording.dat"`)
    **job_kwargs : keyword arguments for parallel processing:
            * chunk_duration or chunk_size or chunk_memory or total_memory
                - chunk_size : int
                    Number of samples per chunk
                - chunk_memory : str
                    Memory usage for each job (e.g. "100M", "1G", "500MiB", "2GiB")
                - total_memory : str
                    Total memory usage (e.g. "500M", "2G")
                - chunk_duration : str or float or None
                    Chunk duration in s if float or with units if str (e.g. "1s", "500ms")
            * n_jobs : int | float
                Number of jobs to use. With -1 the number of jobs is the same as number of cores.
                Using a float between 0 and 1 will use that fraction of the total cores.
            * progress_bar : bool
                If True, a progress bar is printed
            * mp_context : "fork" | "spawn" | None, default: None
                Context for multiprocessing. It can be None, "fork" or "spawn".
                Note that "fork" is only safely available on LINUX systems
